{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Logistic Regression (Theoretical and Practical Implementation)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83c\udf93 Part 1: Theoretical Explanation"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 1. Introduction to Logistic Regression\n", "- **Definition**: Logistic Regression is a statistical model used for binary classification problems. Despite its name, it is used for classification rather than regression tasks.\n", "- **Goal**: Predict the probability that a given input belongs to a particular category (class 0 or class 1)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 2. Why Not Linear Regression for Classification?\n", "- Linear regression outputs continuous values, which can exceed the [0,1] range.\n", "- Classification requires probabilistic interpretation.\n", "- Logistic regression addresses this by using the **sigmoid (logistic) function** to constrain output between 0 and 1."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 3. The Logistic (Sigmoid) Function\n", "$$\n", "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n", "$$\n", "- S-shaped curve.\n", "- Converts linear combination of inputs into a probability.\n", "- Output ranges from 0 to 1."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 4. Model Representation\n", "- Input features: $x = (x_1, x_2, ..., x_n)$\n", "- Parameters: $\\beta = (\\beta_0, \\beta_1, ..., \\beta_n)$\n", "- Linear combination: $z = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n$\n", "- Prediction: $P(y=1|x) = \\sigma(z) = \\frac{1}{1 + e^{-z}}$"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 5. Cost Function: Binary Cross-Entropy (Log-Loss)\n", "$$\n", "L(\\beta) = - \\sum_{i=1}^m \\left[y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i)\\right]\n", "$$\n", "- Measures how well the model's predictions match actual labels.\n", "- Convex, enabling effective optimization using Gradient Descent."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 6. Model Optimization\n", "- **Gradient Descent**: Iteratively updates weights to minimize the loss.\n", "- **Regularization**:\n", "  - L1 (Lasso): Promotes sparsity.\n", "  - L2 (Ridge): Penalizes large coefficients to prevent overfitting."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 7. Decision Boundary\n", "- Class prediction:\n", "$$\n", "y = \\begin{cases} 1 & \\text{if } P(y=1|x) \\geq 0.5 \\\\ 0 & \\text{otherwise} \\end{cases}\n", "$$\n", "- Can be visualized in 2D feature space as a line separating classes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 8. Model Assumptions\n", "- Linearity in the log-odds.\n", "- No multicollinearity.\n", "- Independence of observations."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 9. ROC Curve (Receiver Operating Characteristic)\n", "- **Definition**: A plot of the True Positive Rate (TPR) against the False Positive Rate (FPR) at various threshold settings.\n", "- **True Positive Rate (Recall)**: $TPR = \\frac{TP}{TP + FN}$\n", "- **False Positive Rate**: $FPR = \\frac{FP}{FP + TN}$\n", "- Helps visualize the trade-off between sensitivity and specificity.\n", "- **AUC (Area Under Curve)**: Measures the overall ability of the model to distinguish between classes. AUC close to 1.0 indicates a good model."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### 10. Model Interpretation\n", "- The coefficients $\\beta_i$ indicate the effect of each feature on the log-odds of the outcome.\n", "- To interpret:\n", "  - Convert to **odds ratio**: $OR = e^{\\beta_i}$\n", "  - $OR > 1$: Feature increases the odds of the positive class.\n", "  - $OR < 1$: Feature decreases the odds.\n", "- Useful for understanding feature importance and direction of influence."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcaa Part 2: Practical Implementation (Python)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Import Libraries\n", "import pandas as pd\n", "import numpy as np\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.linear_model import LogisticRegression\n", "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Load Dataset\n", "from sklearn.datasets import load_breast_cancer\n", "\n", "data = load_breast_cancer()\n", "df = pd.DataFrame(data.data, columns=data.feature_names)\n", "df['target'] = data.target\n", "df.head()"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Data Preprocessing\n", "df.isnull().sum()\n", "\n", "X = df.drop('target', axis=1)\n", "y = df['target']\n", "\n", "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Model Training and Evaluation\n", "model = LogisticRegression(max_iter=10000)\n", "model.fit(X_train, y_train)\n", "\n", "y_pred = model.predict(X_test)\n", "print(confusion_matrix(y_test, y_pred))\n", "print(classification_report(y_test, y_pred))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# ROC Curve\n", "y_proba = model.predict_proba(X_test)[:, 1]\n", "fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n", "\n", "plt.plot(fpr, tpr)\n", "plt.xlabel(\"False Positive Rate\")\n", "plt.ylabel(\"True Positive Rate\")\n", "plt.title(\"ROC Curve\")\n", "plt.show()\n", "\n", "print(\"AUC Score:\", roc_auc_score(y_test, y_proba))"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# Model Interpretation\n", "coeff_df = pd.DataFrame({'Feature': X.columns, 'Coefficient': model.coef_[0]})\n", "coeff_df['Odds Ratio'] = np.exp(coeff_df['Coefficient'])\n", "coeff_df.sort_values(by='Odds Ratio', ascending=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## \ud83d\udcc6 Assignment for Students\n", "**Objective**: Apply logistic regression to a new dataset.\n", "1. Choose a binary classification dataset from Kaggle or UCI (e.g., Titanic, Pima Indians Diabetes).\n", "2. Perform EDA (Exploratory Data Analysis).\n", "3. Preprocess the data (handle missing values, encode categories, normalize if needed).\n", "4. Implement logistic regression.\n", "5. Evaluate the model using confusion matrix, precision, recall, F1-score, and ROC curve.\n", "6. Interpret the coefficients.\n", "7. Submit a Jupyter Notebook with your code, plots, and written observations."]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 2}